{
    "name": "inpainting_celebahq", // experiments name
    "gpu_ids": [0], // gpu ids list, default is single 0
    "seed" : -1, // random seed, seed <0 represents randomization not used 
    "finetune_norm": false, // find the parameters to optimize

    "path": { //set every part file path
        "base_dir": "experiments", // base path for all log except resume_state
        "code": "code", // code backup
        "tb_logger": "tb_logger", // path of tensorboard logger
        "results": "results",
        "checkpoint": "checkpoint",
        "resume_state": "/kaggle/input/o64-m8-320/320" 
        // "resume_state": null // ex: 100, loading .state  and .pth from given epoch and iteration
    },

    "datasets": { // train or test
        "train": { 
            "which_dataset": {  // import designated dataset using arguments 
                "name": ["data.dataset", "InpaintDataset"], // import Dataset() class / function(not recommend) from data.dataset.py (default is [data.dataset.py])
                "args":{ // arguments to initialize dataset
                    "data_root": "/kaggle/working/o64_m8_mean/datasets/celebahq/flist/train_64_28000.flist",
                    "data_len": -1,
                    "mask_config": {
                        "mask_mode": "hybrid",
                        "phase": "train",
                        "m12_mode": "mask_generation/lama_generation/random_medium_64.yaml"
                    }
                }
            },
            "dataloader":{
                "validation_split": 4, // percent or number
                "args":{ // arguments to initialize train_dataloader
                    "batch_size": 5, // batch size in each gpu
                    "num_workers": 4,
                    "shuffle": true,
                    "pin_memory": true,
                    "drop_last": true
                },
                "val_args":{ // arguments to initialize valid_dataloader, will overwrite the parameters in train_dataloader
                    "batch_size": 2, // batch size in each gpu
                    "num_workers": 4,
                    "shuffle": false,
                    "pin_memory": true,
                    "drop_last": true
                }
            }
        },
        "test": { 
            "which_dataset": {
                "name": "InpaintDataset", // import Dataset() class / function(not recommend) from default file
                "args":{
                    "data_root": "/kaggle/working/o64_m8_mean/datasets/celebahq/flist/test_64_2000.flist",
                    "mask_config": {
                        "mask_mode": "center",
                        "phase": "test"
                    }
                }
            },
            "dataloader":{
                "args":{
                    "batch_size": 4,
                    "num_workers": 4,
                    "pin_memory": true
                }
            }
        }
    },

    "model": { // networks/metrics/losses/optimizers/lr_schedulers is a list and model is a dict
        "which_model": { // import designated  model(trainer) using arguments 
            "name": ["models.model", "Palette"], // import Model() class / function(not recommend) from models.model.py (default is [models.model.py])
            "args": {
                "sample_num": 8, // process of each image
                "task": "inpainting",
                "ema_scheduler": {
                    "ema_start": 1,
                    "ema_iter": 1,
                    "ema_decay": 0.9999
                },
                "optimizers": [
                    { "lr": 5e-5, "weight_decay": 0}
                ]
            }
        }, 
        "which_networks": [ // import designated list of networks using arguments
            {
                "name": ["models.network", "Network"], // import Network() class / function(not recommend) from default file (default is [models/network.py]) 
                "args": { // arguments to initialize network
                    "init_type": "kaiming", // method can be [normal | xavier| xavier_uniform | kaiming | orthogonal], default is kaiming
                    "module_name": "guided_diffusion", // sr3 | guided_diffusion
                    "unet": {
                        "in_channel": 6,
                        "out_channel": 3,
                        "inner_channel": 64,
                        "channel_mults": [
                            1,
                            2,
                            4,
                            8
                        ],
                        "attn_res": [
                            // 32,
                            16
                            // 8
                        ],
                        "num_head_channels": 32,
                        "res_blocks": 2,
                        "dropout": 0.2,
                        "image_size": 64
                    },
                    "beta_schedule": {
                        "train": {
                            "schedule": "linear",
                            "n_timestep": 2000,
                            // "n_timestep": 10, // debug
                            "linear_start": 1e-6,
                            "linear_end": 0.01
                        },
                        "test": {
                            "schedule": "linear",
                            "n_timestep": 1000,
                            "linear_start": 1e-4,
                            "linear_end": 0.09
                        }
                    }
                }
            }
        ],
        "which_losses": [ // import designated list of losses without arguments
            "mse_loss" // import mse_loss() function/class from default file (default is [models/losses.py]), equivalent to { "name": "mse_loss", "args":{}}
        ],
        "which_metrics": [ // import designated list of metrics without arguments
            "mae" // import mae() function/class from default file (default is [models/metrics.py]), equivalent to { "name": "mae", "args":{}}
        ]
    },

    "train": { // arguments for basic training
        "n_epoch": 354, // max epochs, not limited now
        "n_iter": 1e8, // max interations
        "val_epoch": 1, // valdation every specified number of epochs
        "save_checkpoint_epoch": 10,
        "log_iter": 1e3, // log every specified number of iterations
        "tensorboard" : false, // tensorboardX enable
        "min_val_mae_loss": 0.03360462188720703,
        "min_val_flag": false,
        "train_previous": [0.08530702163933214, 0.018943586379004784, 0.01516380057013667, 0.012444856269423989, 0.01167374264525049, 0.01126680354477048, 0.010210968097782719, 0.010091825282058873, 0.010185467517929935, 0.009679518191387885, 0.009771642708429683, 0.009642332731662763, 0.009105797257525776, 0.009058930832615972, 0.008753284278220788, 0.009051666934557043, 0.008493203205616253, 0.008402547127175749, 0.008600493418694299, 0.008542702455412405, 0.008574075233147084, 0.008455557209524536, 0.008192343588500142, 0.008573005495129427, 0.008420236895892773, 0.00832528084779072, 0.008371570091114012, 0.00796262232529969, 0.008023188426192247, 0.007834295973600573, 0.007867360264243823, 0.007918226711438582, 0.0079217612581748, 0.007594339978140087, 0.007615790078888422, 0.007517094965165706, 0.007851389479172976, 0.0076489039900140545, 0.00745442119285811, 0.007707230871427573, 0.007535447867137573, 0.007478605102916893, 0.0076190208420108246, 0.007769455020932884, 0.0076390109434794915, 0.0077226107569457565, 0.007538126942592277, 0.007670045851333014, 0.007501910877774056, 0.007554097373669189, 0.007653552700775537, 0.007464157708213947, 0.007722448302185039, 0.007566313195469583, 0.007548175260846223, 0.0071077753524904815, 0.007799031569305024, 0.007432472681121699, 0.007498745317138499, 0.00720462700714968, 0.007050606894370997, 0.007442553668199573, 0.007530105543539851, 0.007641688335958961, 0.007332872962745802, 0.007273976854764601, 0.007054361087140993, 0.007497212121768096, 0.007527451673546887, 0.0071049751729741805, 0.0073760814330901025, 0.0072330441840474215, 0.0072696429398205024, 0.007137211071623053, 0.007538212237237732, 0.0075213403503673545, 0.007428932337991625, 0.007324184721066967, 0.0072858155813918, 0.007480400483063476, 0.007081517541708476, 0.007030361962341447, 0.007024807141554206, 0.0069845092712906365, 0.007179502864521427, 0.007191147380794989, 0.007154752593627849, 0.007144356112685116, 0.0067526738664924185, 0.0070570159123838645, 0.00717748556239524, 0.007141682134911593, 0.006740527103978937, 0.007069436293066106, 0.00699686881806866, 0.007017132712419832, 0.006831089618715437, 0.006956257240312124, 0.006857198992528656, 0.006968068963034937, 0.00704310140808734, 0.00708734337572008, 0.007114723522084397, 0.006857408183739423, 0.006571512114911635, 0.006911337757647959, 0.00690542423433584, 0.007104958788120531, 0.006958960658878543, 0.0070428411736044695, 0.006724497148786368, 0.007236825217567625, 0.007222379742985293, 0.0071031487880228545, 0.007169739474831685, 0.007078939481167202, 0.007085538883902056, 0.0074540172066631894, 0.007109616993114195, 0.006934743388000396, 0.007437948796786687, 0.007006451877281817, 0.006924861457129826, 0.006843287676061498, 0.007093932955158504, 0.0070880864235136, 0.007247136438168683, 0.006770295428727996, 0.006763578221123786, 0.0069436374766200185, 0.006907055446206473, 0.006626492911256562, 0.006982604661542019, 0.0068980475586499865, 0.006857898145048766, 0.00694473736849875, 0.007035272081551147, 0.006961431950359458, 0.0073956385570883455, 0.007251013479710424, 0.007014402548435178, 0.006912995165650661, 0.007076360867111022, 0.006603962460461926, 0.006664074365955749, 0.0070944562172397165, 0.006921460257732451, 0.006875413296900411, 0.007041552865837948, 0.006791715652130666, 0.006999143028076507, 0.006668159034109533, 0.006659651935754879, 0.006691781059320699, 0.0066543262289620935, 0.006735923043676964, 0.006861775200794957, 0.006776864247049082, 0.006719456254057774, 0.006633048477305921, 0.006603245348320497, 0.006850077612715163, 0.006676899612558384, 0.006533062230897263, 0.006567923931937305, 0.00670124298505569, 0.006416787302429903, 0.0065526987126859875, 0.006859238020424501, 0.006877532631821957, 0.006620171376468714, 0.006474765463062427, 0.0063632620408620175, 0.006498282994943291, 0.006702469101187737, 0.006473090765582813, 0.0068285215247921175, 0.006913141494985666, 0.006767424970763248, 0.006601414367174121, 0.006785189463168229, 0.00704470088000801, 0.0066677690795736745, 0.006604544477256424, 0.006707821158195745, 0.006717401382758281, 0.0066221696248204434, 0.006761113457920366, 0.00651572418486838, 0.006475425155863437, 0.006467988871087846, 0.0063121862604366055, 0.0063076736682256196, 0.006355398586167007, 0.006434366118122057, 0.0064711204654913585, 0.006481719270928056, 0.006422452827991829, 0.006480623977890297, 0.006484253511690309, 0.00667140226714107, 0.006510254969580933, 0.006579548064038178, 0.006489511210575815, 0.006312011202826335, 0.006565456244496473, 0.006685499466653022, 0.006876811960252603, 0.006406566949936404, 0.006560120098209922, 0.006386463265168927, 0.006509108001951852, 0.006373191216350626, 0.006660428807164211, 0.006586060633795079, 0.00633375539464742, 0.00663695899654486, 0.006055163866103282, 0.006348393357625322, 0.006234737783093641, 0.006745667596970725, 0.006409659848171842, 0.006635565502453873, 0.0066490728709199575, 0.006550024157212041, 0.006284041380969403, 0.006597749671157076, 0.006401893734556325, 0.006330443553784776, 0.006516276829890804, 0.006464614430988303, 0.0064076911024232115, 0.006479243894348649, 0.006179251118176645, 0.006504758971132156, 0.006338565935868174, 0.00639005502604327, 0.006553723320459648, 0.006349410092773551, 0.006509049689092522, 0.006689529424383135, 0.006215591357196015, 0.006388978946731489, 0.006239139786830208, 0.006387702430739041, 0.006498360225779084, 0.006342883672681093, 0.006062479057377005, 0.00646600112136224, 0.006332582175349257, 0.006348393357625322, 0.006234737783093641, 0.006745667596970725, 0.006409659848171842, 0.006635565502453873, 0.0066490728709199575, 0.006550024157212041, 0.006284041380969403, 0.006597749671157076, 0.006401893734556325, 0.006330443553784776, 0.006516276829890804, 0.006464614430988303, 0.0064076911024232115, 0.006479243894348649, 0.006179251118176645, 0.006504758971132156, 0.006338565935868174, 0.00639005502604327, 0.006553723320459648, 0.006349410092773551, 0.006509049689092522, 0.006689529424383135, 0.006215591357196015, 0.006388978946731489, 0.006239139786830208, 0.006387702430739041, 0.006498360225779084, 0.006342883672681093, 0.006062479057377005, 0.00646600112136224, 0.006332582175349257, 0.0061207211323765975, 0.006348393357625322, 0.006234737783093641, 0.006745667596970725, 0.006409659848171842, 0.006635565502453873, 0.0066490728709199575, 0.006550024157212041, 0.006284041380969403, 0.006597749671157076, 0.006401893734556325, 0.006330443553784776, 0.006516276829890804, 0.006464614430988303, 0.0064076911024232115, 0.006479243894348649, 0.006179251118176645, 0.006504758971132156, 0.006338565935868174, 0.00639005502604327, 0.006553723320459648, 0.006349410092773551, 0.006509049689092522, 0.006689529424383135, 0.006215591357196015, 0.006388978946731489, 0.006239139786830208, 0.006387702430739041, 0.006498360225779084, 0.006342883672681093, 0.006062479057377005, 0.00646600112136224, 0.006332582175349257, 0.0061207211323765975, 0.006524327034713743, 0.00643190866661229, 0.006423590294501515, 0.0065860791488766045, 0.0063789830303032265, 0.0062835604902458314, 0.006420449859143832, 0.0066472265993912, 0.0062845679520635135, 0.006391951931826176, 0.006384983074462633, 0.006514048183757366, 0.006396496958954905, 0.006067227975881738, 0.006293225876058406, 0.0064543349982546065, 0.006567584657522362, 0.0061539777314580215, 0.006155801074626947, 0.006524200607963249, 0.006629646140399386, 0.0064758463077840625, 0.00656401360896835, 0.006261488271568595, 0.006491581266567879, 0.006089250918391856, 0.006231442096367443, 0.006260653090630858, 0.00631448845165104, 0.006256405664213452, 0.006145635789712002, 0.00604759392149121, 0.006323626291737064, 0.006392108895081115, 0.0061121201634503864, 0.006294649755103807, 0.0064995540547265855, 0.005895462167548201, 0.006405269249048473, 0.006294882199567155, 0.006563890884858292, 0.00642849333729488, 0.006563918096096619, 0.006146290477751272, 0.006379090718302886, 0.00649023886905811, 0.005885748373092926, 0.0065000122903316575, 0.006483072361756073, 0.006290985138995806, 0.006153338572889157, 0.00617871548728605, 0.006302623416878298, 0.006186663691343826, 0.006150877824682519, 0.006232704416551849, 0.00599823254617851, 0.006143418311979254, 0.006399589074217483, 0.006256926390184464, 0.006272424528180932, 0.0064946043934738205, 0.0063281423556083094, 0.006251779021671985, 0.006431799216023462, 0.006031463448932635, 0.0062139679042915235, 0.0064976398357323405, 0.006028111102325311],
        "eval_previous": [0.11468203365802765, 0.10017223656177521, 0.07366480678319931, 0.07317798584699631, 0.12783637642860413, 0.10984153300523758, 0.05958137661218643, 0.05302033573389053, 0.06083337590098381, 0.06447456777095795, 0.055266834795475006, 0.1155867725610733, 0.05502001941204071, 0.05927959829568863, 0.07080069929361343, 0.058640237897634506, 0.066536083817482, 0.06062537431716919, 0.05947890505194664, 0.05564075708389282, 0.05636551231145859, 0.0565715916454792, 0.054245464503765106, 0.049492329359054565, 0.05137290060520172, 0.05399695783853531, 0.05281530320644379, 0.06527166068553925, 0.04917828366160393, 0.05031750351190567, 0.047581762075424194, 0.04943937063217163, 0.053928423672914505, 0.042452335357666016, 0.04903404787182808, 0.047421492636203766, 0.060263246297836304, 0.046007994562387466, 0.05076795071363449, 0.04535587877035141, 0.05931863188743591, 0.042415518313646317, 0.05281050503253937, 0.050875626504421234, 0.04757793992757797, 0.05427030473947525, 0.048650652170181274, 0.04955814406275749, 0.048981182277202606, 0.0582209974527359, 0.04550500586628914, 0.053705472499132156, 0.043822214007377625, 0.0549534410238266, 0.05016046762466431, 0.0457431823015213, 0.05041343718767166, 0.05065238103270531, 0.04965834692120552, 0.048341304063797, 0.052711598575115204, 0.045326344668865204, 0.04510311409831047, 0.05460619926452637, 0.04405521973967552, 0.05219095200300217, 0.043169230222702026, 0.06252530217170715, 0.045698270201683044, 0.046945810317993164, 0.04766659438610077, 0.04372432827949524, 0.044811323285102844, 0.0483541414141655, 0.0472756065428257, 0.04350985586643219, 0.04281426593661308, 0.04597213864326477, 0.0467965230345726, 0.04028838500380516, 0.048827286809682846, 0.04275129735469818, 0.04724154621362686, 0.04761607199907303, 0.040329478681087494, 0.0529518760740757, 0.046956464648246765, 0.047432854771614075, 0.04532071575522423, 0.04383321851491928, 0.03890044242143631, 0.06309149414300919, 0.04643947631120682, 0.04339037835597992, 0.05689345300197601, 0.04328112676739693, 0.04799502342939377, 0.0571010522544384, 0.05033662170171738, 0.04194650799036026, 0.04689797759056091, 0.049477044492959976, 0.04604969173669815, 0.043890051543712616, 0.043158166110515594, 0.04823707789182663, 0.048702917993068695, 0.04637365788221359, 0.052032507956027985, 0.04206904396414757, 0.041659802198410034, 0.037907302379608154, 0.04836830496788025, 0.04891069978475571, 0.05096862465143204, 0.039372652769088745, 0.04916054755449295, 0.04339621961116791, 0.0415547639131546, 0.04799043387174606, 0.045865003019571304, 0.04880697280168533, 0.03836844861507416, 0.04364345967769623, 0.045072708278894424, 0.04383694380521774, 0.047153081744909286, 0.041327230632305145, 0.04005565866827965, 0.04464038088917732, 0.04040177911520004, 0.0424027219414711, 0.039505667984485626, 0.044840216636657715, 0.04349280521273613, 0.04439697787165642, 0.03987622261047363, 0.03771746903657913, 0.04311995208263397, 0.04320356249809265, 0.04706022888422012, 0.042257655411958694, 0.04537443071603775, 0.04337893798947334, 0.04665065556764603, 0.042293041944503784, 0.036887526512145996, 0.045510806143283844, 0.04794769734144211, 0.04328527674078941, 0.04911232739686966, 0.0364513024687767, 0.039424337446689606, 0.04401271045207977, 0.043357960879802704, 0.04811744764447212, 0.04326717555522919, 0.04569384083151817, 0.04748425632715225, 0.04765214025974274, 0.04413358122110367, 0.04271022230386734, 0.035469312220811844, 0.03912821784615517, 0.0374433696269989, 0.06318783760070801, 0.047257743775844574, 0.04302824288606644, 0.039381250739097595, 0.04033034294843674, 0.044103190302848816, 0.040771014988422394, 0.04986045882105827, 0.04427589476108551, 0.03939467668533325, 0.04114615172147751, 0.04324819892644882, 0.046796515583992004, 0.044796645641326904, 0.04216278716921806, 0.042779773473739624, 0.04867762699723244, 0.041874002665281296, 0.03913787007331848, 0.04195702075958252, 0.04209575057029724, 0.045917659997940063, 0.04927872121334076, 0.05232444405555725, 0.042705219238996506, 0.04311653971672058, 0.04445416107773781, 0.046508222818374634, 0.04707222059369087, 0.04236902296543121, 0.04928123950958252, 0.04391222447156906, 0.04009239375591278, 0.0450306311249733, 0.047338131815195084, 0.040361132472753525, 0.044264379888772964, 0.04735495150089264, 0.04516434669494629, 0.04073427990078926, 0.04303622990846634, 0.03876345977187157, 0.04289283603429794, 0.04396160691976547, 0.048998091369867325, 0.044476162642240524, 0.045403651893138885, 0.042132534086704254, 0.04242895543575287, 0.03856538608670235, 0.03951280564069748, 0.037230171263217926, 0.038194090127944946, 0.04011918604373932, 0.046375781297683716, 0.042641520500183105, 0.04132598266005516, 0.04121179133653641, 0.049483925104141235, 0.04389946907758713, 0.045182276517152786, 0.03882664442062378, 0.04409751296043396, 0.04600206017494202, 0.04738890379667282, 0.04388149082660675, 0.03893328458070755, 0.0435420498251915, 0.04103655368089676, 0.043109726160764694, 0.04748034104704857, 0.04683878272771835, 0.045177705585956573, 0.04255808889865875, 0.04363445192575455, 0.0412319079041481, 0.03907601535320282, 0.047630347311496735, 0.04354112595319748, 0.042485568672418594, 0.043563272804021835, 0.03837346285581589, 0.04132641851902008, 0.03646745905280113, 0.04328695684671402, 0.04011918604373932, 0.046375781297683716, 0.042641520500183105, 0.04132598266005516, 0.04121179133653641, 0.049483925104141235, 0.04389946907758713, 0.045182276517152786, 0.03882664442062378, 0.04409751296043396, 0.04600206017494202, 0.04738890379667282, 0.04388149082660675, 0.03893328458070755, 0.0435420498251915, 0.04103655368089676, 0.043109726160764694, 0.04748034104704857, 0.04683878272771835, 0.045177705585956573, 0.04255808889865875, 0.04363445192575455, 0.0412319079041481, 0.03907601535320282, 0.047630347311496735, 0.04354112595319748, 0.042485568672418594, 0.043563272804021835, 0.03837346285581589, 0.04132641851902008, 0.03646745905280113, 0.04328695684671402, 0.03757692873477936, 0.04011918604373932, 0.046375781297683716, 0.042641520500183105, 0.04132598266005516, 0.04121179133653641, 0.049483925104141235, 0.04389946907758713, 0.045182276517152786, 0.03882664442062378, 0.04409751296043396, 0.04600206017494202, 0.04738890379667282, 0.04388149082660675, 0.03893328458070755, 0.0435420498251915, 0.04103655368089676, 0.043109726160764694, 0.04748034104704857, 0.04683878272771835, 0.045177705585956573, 0.04255808889865875, 0.04363445192575455, 0.0412319079041481, 0.03907601535320282, 0.047630347311496735, 0.04354112595319748, 0.042485568672418594, 0.043563272804021835, 0.03837346285581589, 0.04132641851902008, 0.03646745905280113, 0.04328695684671402, 0.03757692873477936, 0.04228178784251213, 0.039981767535209656, 0.041073381900787354, 0.044345270842313766, 0.03923897445201874, 0.04351222515106201, 0.0431186817586422, 0.04101717472076416, 0.03912966698408127, 0.044936418533325195, 0.04022175818681717, 0.040283072739839554, 0.04162939265370369, 0.0401480495929718, 0.04007609933614731, 0.0418718196451664, 0.039791107177734375, 0.04240720719099045, 0.04283057898283005, 0.047457337379455566, 0.042258523404598236, 0.04077262058854103, 0.03360462188720703, 0.03575875610113144, 0.041821446269750595, 0.035895802080631256, 0.045591726899147034, 0.039344850927591324, 0.039537109434604645, 0.04622891917824745, 0.03890858590602875, 0.04322485625743866, 0.04912219196557999, 0.04306737333536148, 0.0425843670964241, 0.03704526275396347, 0.043685875833034515, 0.03600659221410751, 0.03835161775350571, 0.04145609214901924, 0.04905488342046738, 0.04252264276146889, 0.04060959815979004, 0.04083758965134621, 0.036880508065223694, 0.03922145068645477, 0.039827074855566025, 0.040627725422382355, 0.04146292060613632, 0.04079344868659973, 0.040189191699028015, 0.03545849397778511, 0.04359487444162369, 0.04523106664419174, 0.0378122478723526, 0.044380899518728256, 0.04304002970457077, 0.04260139539837837, 0.042932186275720596, 0.0436154305934906, 0.039553843438625336, 0.04319479316473007, 0.036734312772750854, 0.04355962947010994, 0.04015956446528435, 0.04181668907403946, 0.04445820301771164, 0.04125143587589264, 0.04717681556940079]
    },
    
    "debug": { // arguments in debug mode, which will replace arguments in train
        "val_epoch": 1,
        "save_checkpoint_epoch": 1,
        "log_iter": 2,
        "debug_split": 50 // percent or number, change the size of dataloder to debug_split.
    }
}
